<!DOCTYPE html>
<html lang="en">

<head>
      <title>SwanLotus - Eigenvalues and Eigenvectors---Why Are They Important?</title>
    <meta charset="utf-8">
    <meta name="generator" content="Pelican">

    <!--Required for site to be responsive-->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Speeds up loading of Google Fonts-->
    <link rel="dns-prefetch" href="//fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

    <!--Stylesheet-->
    <link rel="stylesheet" href="http://localhost:8000/theme/css/swanlotus.css">
    <link rel="shortcut icon" href="http://localhost:8000/images/favicon.ico" type="image/x-icon"/>

    <!--Font Awesome-->
    <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>

    <!--MathJax-->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
    <!--[if lt IE 9]>
        <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
    <![endif]-->

    <!--jQuery-->
    <script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
    <script>
        $(document).ready(function() {

            // Check for click events on the navbar burger icon
            $(".navbar-burger").click(function() {

                // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                $(".navbar-burger").toggleClass("is-active");
                $(".navbar-menu").toggleClass("is-active");

            });
        });
    </script>



</head>
<body>
    <div class="container">
        <!--Navigation Bar-->
        <nav class="navbar" role="navigation" aria-label="main navigation">
            <div class="navbar-brand">
                <a class="navbar-item" href="http://localhost:8000">
                    <img src="http://localhost:8000/images/swanlotus-brand.svg" alt="SwanLotus">
                </a>

                <!--Collapsed menu for small screens-->
                <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navMenu" class="navbar-menu">
                <div class="navbar-start">
                                    <a class="navbar-item" href="http://localhost:8000/index.html">
                                        Home
                                    </a>
                                    <a class="navbar-item" href="http://localhost:8000/sas.html">
                                        Secrets of Academic Success
                                    </a>
                                    <a class="navbar-item current" href="http://localhost:8000/blog.html">
                                        Blogs
                                    </a>
                <div class="navbar-end"></div>
            </div>
        </nav>
        <div id="two-column" class="tile is-parent">
            <!--Main Column-->
            <div id="main" class="tile is-child is-9 is-size-7-mobile is-size-5-tablet is-size-4-desktop content">
  <header>
    <h1 id="Eigenvalues and Eigenvectors---Why Are They Important?">Eigenvalues and Eigenvectors---Why Are They Important?</h1>
    <p class="author">R (Chandra) Chandrasekhar</p>
      <p class="date">2015-12-13</p>
  </header>
  <p>A university academic friend of mine recently remarked that it was not easy to motivate students to study <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors">eigenvalues and eigenvectors,</a> let alone appreciate their importance: the subject itself was abstract, and the applications tended to be domain-specific and somewhat arcane.</p>
<p>A cursory Web search turned up results that confirmed his assertions and concerns. There have been <a href="http://matheducators.stackexchange.com/questions/520/what-is-a-good-motivation-showcase-for-a-student-for-the-study-of-eigenvalues">pro</a> and <a href="http://matheducators.stackexchange.com/questions/8586/too-much-motivation">con</a> views about motivating students to learn about eigenvalues and eigenvectors, and especially to convey <a href="http://matheducators.stackexchange.com/questions/3983/what-is-the-best-way-to-intuitively-explain-what-eigenvectors-and-eigenvalues-ar">intuitively</a> their <a href="http://math.stackexchange.com/questions/23312/what-is-the-importance-of-eigenvalues-eigenvectors">importance</a>.</p>
<p>I then asked, “Can I explain eigenvalues, eigenvectors, and their importance to <em>myself?”</em>. It also occurred to me that the harried and hurried students of today might derive some benefit from my efforts; hence this blog. It is a brief, largely qualitative, and mathematically non-rigorous article on eigenvalues and eigenvectors that aims to provide meaning and motivation for their study. Corrections and suggestions for improvement are most welcome. :-)</p>
<h2 id="eigenvalues-and-eigenvectors">Eigenvalues and eigenvectors</h2>
<p>As a general rule, the more powerful an idea, the more prevalent it becomes. Think about words and numbers, and you will see what I mean.</p>
<p>Eigenvalues and eigenvectors are one such powerful idea. It is no surprise that they appear in different guises in different contexts: in oscillating electronic circuits, in dynamical systems, in computer games, in the spectra of atoms, and in Google searches, to name just a few.</p>
<p>The word <a href="https://en.wikipedia.org/wiki/Talk:Eigenvector"><em>eigen</em></a> is German in origin and means “inherent, characteristic, natural, own, or peculiar (to)”. So the prefix “eigen” captures the natural essence of the noun it qualifies. Perhaps the word “idiosyncratic” comes closest to conveying its import.</p>
<h2 id="matrices">Matrices</h2>
<p>Eigenvalues and eigenvectors are associated traditionally with <a href="https://en.wikipedia.org/wiki/Matrix_%28mathematics%29"><em>matrices</em></a>. If numbers are like tea-leaves, matrices are like tea-bags. They are rectangular arrays of numbers, whether real or complex, that have been hijacked by mathematicians to serve as a shorthand in a variety of contexts. What they mean depends on context and level of abstraction. They can represent geometric transformations in Euclidean space, or systems of linear equations, or systems of linear differential equations with constant coefficients, or linear transformations in vector spaces. Note the recurrence of the word <em>linear</em> here.</p>
<h2 id="invariance-and-identity-elements">Invariance and identity elements</h2>
<p><a href="http://mathworld.wolfram.com/Invariant.html">Invariance</a> is a central concept in mathematics and physics. Adding zero to a number leaves it unchanged. Multiplying a number by one again leaves it unchanged. And zero and one are important numbers, usually called the <em>additive</em> and <em>multiplicative identity elements</em> respectively. Consider now the matrix equivalent of multiplying by <span class="math display">\[1\]</span>, an example of which is:</p>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 0\\0 &amp; 1\end{bmatrix}\begin{bmatrix}v_{1}\\v_{2}\end{bmatrix}
= 1\begin{bmatrix}v_{1}\\v_{2}\end{bmatrix} = \begin{bmatrix}v_{1}\\v_{2}\end{bmatrix}
\qquad (1)
\]</span></p>
<p>The <span class="math display">\[2 \times 2\]</span> matrix on the extreme left of equation (<a href="#eq:identity">1</a>) is the <em>identity matrix</em> of dimension <span class="math display">\[2\]</span>, analogous to the multiplicative identity. We could write this equation more succinctly as:</p>
<p><span class="math display">\[ I\mathbf{v} = 1\mathbf{v} \qquad (2)\]</span></p>
<p><span class="math display">\[I\]</span>, on the left, is the <em>identity matrix</em>, the number <span class="math display">\[1\]</span> on the right is called an <em>eigenvalue</em> and the vector <span class="math display">\[\mathbf{v}\]</span> is called an <em>eigenvector</em>. Note that there are no strictures on <span class="math display">\[\mathbf{v}.\]</span> So, in this particular case, <em>all</em> vectors <span class="math display">\[\mathbf{v}\]</span> are eigenvectors but there is only <em>one</em> eigenvalue, namely <span class="math display">\[1.\]</span> This example, however, is both unusual and contrived, because the identity matrix is a <em>special</em> type of <em>square matrix</em> with ones on its principal diagonal and zeros elsewhere.</p>
<p>Equation (<a href="#eq:succinct">2</a>) is a particular case of the general equation for eigenvalues and eigenvectors, which is written:</p>
<p><span class="math display">\[
M\mathbf{v} = \lambda \mathbf{v}
\qquad (3)\]</span></p>
<p>where <span class="math display">\[M\]</span> is a <em>general square matrix</em>, <span class="math display">\[{\lambda}\]</span> is a <em>real or complex scalar</em> called an <em>eigenvalue</em>, and <span class="math display">\[\mathbf{v}\]</span> is a <em>non-zero vector</em> called an <em>eigenvector</em>. The matrix <span class="math display">\[M\]</span> is assigned meaning according to context. Geometrically, the <em>orientation</em> of the vector <span class="math display">\[\mathbf{v}\]</span> is unchanged by the transformation <span class="math display">\[M\]</span>, although if <span class="math display">\[\lambda\]</span> is negative, the direction is reversed. Specifically, the eigenvector corresponding to an eigenvalue of <span class="math display">\[1\]</span> remains unchanged by the transformation: an example of invariance.</p>
<h2 id="calculus">Calculus</h2>
<p>The operation of taking a derivative may be denoted by the <em>differential operator,</em> <span class="math display">\[D.\]</span> We know that</p>
<p><span class="math display">\[
\frac{d}{dt}e^{t} = D(e^{t}) = e^{t}
\]</span> and further that <span class="math display">\[
\frac{d}{dt}e^{st} = D(e^{st}) = se^{st}
\]</span></p>
<p>where <span class="math display">\[s\]</span> is a scalar and <span class="math display">\[t\]</span> is the independent variable, usually time. Although <span class="math display">\[D\]</span> is not a matrix here, it is nevertheless a linear transformation that operates on <em>functions.</em> And in place of a vector, we have a function, <span class="math display">\[e^{st}\]</span>, which is therefore called an <em>eigenfunction</em> of the differential operator, with eigenvalue <span class="math display">\[s.\]</span> The importance of the complex exponentials in signal and system analysis cannot be over-emphasized: just recall the <a href="https://en.wikipedia.org/wiki/Laplace_transform">Laplace</a> and <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier</a> transforms.</p>
<h2 id="differential-equations">Differential Equations</h2>
<p>Linear homogeneous <em>differential equations</em> with constant coefficients may be written using the <span class="math display">\[D\]</span> notation already introduced. A second order homogeneous equation with independent variable <span class="math display">\[t\]</span> and dependent variable <span class="math display">\[y\]</span> may be written as <span class="math display">\[a_2D^2(y) + a_1D(y) + a_0(y) = 0.\]</span> Plugging in a solution of the form <span class="math display">\[y = e^{st}\]</span>, we get <span class="math display">\[(a_2s^2 + a_1s + a_0)e^{st} = 0.\]</span> Since <span class="math display">\[e^{st}\]</span> can never be zero, we may divide by it to get the <em>characteristic polynomial</em></p>
<p><span class="math display">\[
a_2s^2 + a_1s + a_0 = 0.
\qquad (4)\]</span></p>
<p>The <em>roots</em> of this characteristic polynomial give us the eigenvalues of the system. Perhaps, the prefix “eigen” came to be used because of the adjective “characteristic”.</p>
<p>These ideas masquerade under different terminology in linear system and control theory where <a href="https://en.wikipedia.org/wiki/Transfer_function">transfer function</a>, <a href="http://web.mit.edu/2.14/www/Handouts/PoleZero.pdf">poles and zeros</a>, <a href="https://www.youtube.com/watch?v=hcXbyS2Cf2o">natural frequency and resonance</a>, and <a href="https://en.wikibooks.org/wiki/Control_Systems/Stability">stability</a> are encountered.</p>
<h2 id="characteristic-polynomial-of-a-square-matrix">Characteristic polynomial of a square matrix</h2>
<p>The characteristic polynomial of a square matrix is obtained likewise. The equation <span class="math display">\[M\mathbf{v} = \lambda \mathbf{v}\]</span> may be re-written as <span class="math display">\[(M -\lambda I)\mathbf{v}=\mathbf{0}\]</span>, where the right hand side is the <em>zero vector</em>. Since the eigenvector <span class="math display">\[\mathbf{v}\]</span> is non-zero, this implies that the matrix <span class="math display">\[(M -\lambda I)\]</span> is <a href="https://en.wikipedia.org/wiki/Invertible_matrix">singular</a> or non-invertible, which in turn implies that its <a href="https://en.wikipedia.org/wiki/Determinant">determinant</a> is zero. So, the characteristic polynomial is the equation</p>
<p><span class="math display">\[\det(M - \lambda I) = 0
\qquad (5)\]</span></p>
<p>and its roots are the eigenvalues of <span class="math display">\[M.\]</span> The determinant of a square matrix is a <em>number</em> associated with it, obtained by adding and subtracting products of its elements in a specific order.</p>
<h2 id="linear-transformations-and-vector-spaces">Linear transformations and vector spaces</h2>
<p><a href="https://en.wikipedia.org/wiki/Vector_space">Vector spaces</a> are a powerful mathematical abstraction that allows us to unify many disparate branches of mathematics under a uniform taxonomy. <a href="http://mathworld.wolfram.com/LinearTransformation.html">Linear transformations</a> are a particular type of mapping between two vector spaces over a scalar field, satisfying:</p>
<p><span class="math display">\[
\begin{aligned}
T(\mathbf{u} + \mathbf{v}) &amp;= T(\mathbf{u}) + T(\mathbf{v})\\
T(\alpha\mathbf{u}) &amp;= \alpha T(\mathbf{u})\\
\end{aligned}
\qquad (6)\]</span></p>
<p>where <span class="math display">\[T\]</span> is the transformation, <span class="math display">\[\mathbf{u}\]</span> and <span class="math display">\[\mathbf{v}\]</span> are two vectors, and <span class="math display">\[\alpha\]</span> is a scalar.</p>
<p><span class="math display">\[T\]</span> may be represented by a matrix, and under certain conditions, its eigenvalues and eigenvectors can characterize the transformation <em>completely.</em> This happens when (a) the eigenvectors are <em>linearly independent,</em> i.e., no two eigenvectors are parallel, and (b) when they <em>span</em> the vector space, i.e., any vector within the space can be constructed from a linear combination of the eigenvectors. The eigenvectors are then said to form a <a href="https://en.wikipedia.org/wiki/Basis_%28linear_algebra%29"><em>basis</em></a> for the space.</p>
<p>As a case in point, let us say <span class="math display">\[T\]</span> is a <span class="math display">\[3 \times 3\]</span> matrix whose eigenvectors <span class="math display">\[\mathbf{e_{1}}\]</span>, <span class="math display">\[\mathbf{e_{2}}\]</span>, and <span class="math display">\[\mathbf{e_{3}}\]</span> are linearly independent and form a basis. Then, if <span class="math display">\[\mathbf{v} = \alpha_{1}\mathbf{e_{1}} + \alpha_{2}\mathbf{e_{2}} + \alpha_{3}\mathbf{e_{3}}\]</span>, where the <span class="math display">\[\alpha_{i}\]</span>s are scalars, by virtue of the fact that <span class="math display">\[T\]</span> is a linear transformation, we have</p>
<p><span class="math display">\[
\begin{aligned}
T(\mathbf{v}) &amp; = T(\alpha_{1}\mathbf{e_{1}} + \alpha_{2}\mathbf{e_{2}} + \alpha_{3}\mathbf{e_{3}})\\
&amp; = T(\alpha_{1}\mathbf{e_{1}}) + T(\alpha_{2}\mathbf{e_{2}}) + T(\alpha_{3}\mathbf{e_{3}})\\
&amp; = \alpha_{1}T(\mathbf{e_{1}}) + \alpha_{2}T(\mathbf{e_{2}}) + \alpha_{3}T(\mathbf{e_{3}})\\
&amp; = \alpha_{1}\lambda{_1}\mathbf{e_{1}} + \alpha_{2}\lambda_{2}\mathbf{e_{2}} + \alpha_{3}\lambda_{3}\mathbf{e_{3}}
\end{aligned}
\]</span></p>
<p>Notice how the right hand side is now expressed <em>purely as a sum of scaled eigenvectors.</em> This is the essence of why eigenvalues and eigenvectors are so important: they are <em>sufficient</em> to describe what is taking place. <em>Eigenvalues and eigenvectors encode the transformation succinctly, just as DNA encodes biological information.</em></p>
<p>If, in addition, <span class="math display">\[T\]</span> is <a href="https://en.wikipedia.org/wiki/Symmetry_in_mathematics"><em>symmetric</em></a> its eigenvectors form an <a href="https://en.wikipedia.org/wiki/Orthonormality"><em>orthonormal basis</em></a>. Such basis vectors confer <em>parsimony</em> (think low bandwidth) when images or audio signals need to be deconstructed for transmission and reconstructed on reception. Eigenvectors are also useful in techniques like <a href="https://en.wikipedia.org/wiki/Principal_component_analysis"><em>principal component analysis</em></a> which is used in statistical pattern recognition.</p>
<p>The applications of eigenvalues and eigenvectors in linear algebra run far and deep. Suffice it here to merely mention that an extension, fortuitously called <a href="https://en.wikipedia.org/wiki/Spectral_theory"><em>spectral theory,</em></a> even explains the observed spectra of atoms in quantum theory!</p>
<!--In this section I have given some links to reference material that is freely available online. As with everything on the Web, please check for quality and accuracy before accepting something as correct.-->
<h3 id="a-property-of-eigenvectors">A property of eigenvectors</h3>
<p>I will here belabour a point that might seem blindingly obvious to some but frustratingly obscure to others. Let <span class="math display">\[\mathbf{v}\]</span> be an eigenvector associated with a distinct eigenvalue <span class="math display">\[\lambda\]</span> as in equation (<a href="#eq:eigen">3</a>), and let <span class="math display">\[k\]</span> be a non-zero scalar. Then, using the second of the equation-pair (<a href="#eq:linear">6</a>), we have</p>
<p><span class="math display">\[
M(k\mathbf{v}) = k(M\mathbf{v}) = k(\lambda\mathbf{v}) = \lambda(k\mathbf{v}),
\qquad (7)\]</span></p>
<p>which means that if <span class="math display">\[\mathbf{v}\]</span> is an eigenvector, any non-zero scalar multiple of <span class="math display">\[\mathbf{v}\]</span> is also an eigenvector for that same eigenvalue. So, strictly speaking, we really should be referring to <em>an</em> eigenvector—rather than <em>the</em> eigenevctor—corresponding to any given eigenvalue.</p>
<h2 id="worked-example">Worked example</h2>
<p>A worked example would normally have made its way here at this point in the article. But because the example is long and might not interest everyone, I have relegated it to the end of the article. Stay tuned if you are enthused.</p>
<h2 id="resources">Resources</h2>
<p>I hope that this article has not been so brief as to be cryptic and off-putting. To those in search of greater rigour or a more formal exposition, I would recommend a good linear algebra textbook. The venerable tome that I used at university went by the acronym “KKOP” after the initials of the surnames of the four authors. Fortunately, it is freely available online in a variety of formats, and its actual title is <a href="https://archive.org/details/AnIntroductionToLinearAnalysis"><em>An Introduction to Linear Analysis.</em></a></p>
<p>For something more contemporary, I would recommend the textbooks and lectures of Professor Gilbert Strang of MIT. They are attuned to those who <em>apply</em> mathematics, like engineers and scientists. There is an <a href="https://archive.org/details/MIT18.06S05_MP4">archived video of his lecture on eigenvalues and eigenvectors.</a> There are also links to <a href="http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">his MIT Open Course Ware (OCW) page for Course 18.06 of Spring 2010</a>, <a href="http://math.mit.edu/~gs/linearalgebra/">his linear algebra textbook home page</a>, and <a href="http://www-math.mit.edu/~gs/">his academic home page.</a></p>
<p>Many academics make their lecture notes freely available online: <a href="https://www.google.com/">google</a> for them. <a href="https://www.youtube.com/">You Tube</a> videos of lectures are another source of information and knowledge, which offer the immediacy of a classroom lecture with the convenience of instant rewind in case you need to catch something you missed.</p>
<p>Online forums offer a slightly more interactive learning experience but again, their depth and quality varies. The <a href="http://math.stackexchange.com/">Mathematics StackExchange</a> and <a href="https://www.quora.com/">Quora</a> are two sites that you might explore.</p>
<!--[aops]: http://artofproblemsolving.com/-->
<p>Examples of all the above types of resources have been tucked away within the various links in this article: try them out to get a flavour of what is available.</p>
<h2 id="importance-and-applications">Importance and applications</h2>
<p>If, after all this, you are still unconvinced about the utility of eigenvalues and eigenvectors, think of this analogy. Crystals have natural <a href="https://en.wikipedia.org/wiki/Cleavage_%28crystal%29"><em>cleavage planes</em></a> that allow them to be fractured easily along specific directions. This exploits the <em>symmetry</em> in the crystals. Likewise, eigenvalues and eigenvectors exploit the naturally occurring symmetries of mathematical structures and transformations to allow us to view them more simply and insightfully. Without eigenvalues and eigenvectors, we would have neither radios nor lasers.</p>
<p>To get an idea of the broad sweep of eigenvalues and their applicability, I strongly recommend that you should read a charming article entitled <a href="http://people.maths.ox.ac.uk/trefethen/dec11.pdf">“Favourite Eigenvalue Problems”.</a> Another article that takes a breezy look at the subject of this writeup is <a href="http://hubpages.com/education/What-the-Heck-are-Eigenvalues-and-Eigenvectors">“What the Heck are Eigenvalues and Eigenvectors?”.</a> It has a disputed explanation (see comments on the article) of how a bridge collapsed: so take that <em>cum granis salis.</em> It also contains a link to a PDF paper interestingly entitled <a href="http://www.rose-hulman.edu/~bryan/googleFinalVersionFixed.pdf">“The 25,000,000,000.00 Dollar Eigenvector: The Linear Algebra Behind Google”,</a> which, in good faith, I think is not a spoof! Indeed, the <a href="http://ilpubs.stanford.edu:8090/422/">citation to the original Stanford InfoLab technical report</a> and the <a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf">actual report</a> are both available online.</p>
<h2 id="worked-example-modelling-weather-with-a-transition-matrix">Worked example: modelling weather with a transition matrix</h2>
<p>Now for the promised example of eigenvalues at work—in a simplified real-life situation, modelling the weather. Let us assume that yesterday’s weather influences the <em>probability</em> of today’s weather, and today’s weather influences the <em>probability</em> of tomorrow’s weather. Each day’s weather depends only on the previous day’s weather, i.e., the weather has a “memory” of one day.</p>
<p>To keep it simple, let us have only three weather states: sunny, cloudy, and rainy, with the stipulation that each day can only be <em>one</em> of these three. Further, in our matrix, let the ordering be sunny, cloudy, and rainy, both left to right, and top to bottom. Then, the column headings represent <em>today’s weather</em> and the row headings represent <em>tomorrow’s weather.</em> We then have the <a href="https://en.wikipedia.org/wiki/Stochastic_matrix"><em>state-transition matrix</em></a> or <em>Markov matrix</em> <span class="math display">\[M\]</span> given in equation (<a href="#eq:state">8</a>):</p>
<p><span class="math display">\[
M = \begin{bmatrix}%
0.65 &amp; 0.30 &amp; 0.10\\
0.25 &amp; 0.45 &amp; 0.40\\
0.10 &amp; 0.25 &amp; 0.50\\
\end{bmatrix}
\qquad (8)\]</span></p>
<p>Note that each <em>column</em> of <span class="math display">\[M\]</span> represents the probabilities of <a href="https://en.wikipedia.org/wiki/Mutual_exclusivity"><em>mutually exclusive</em></a> events, which must therefore sum to one. The matrix element <span class="math display">\[m_{ij}\]</span> is the probability that today’s weather is in column <span class="math display">\[j\]</span> and that tomorrow’s weather is in row <span class="math display">\[i.\]</span> For example, The probability of today being rainy and tomorrow cloudy is given by <span class="math display">\[m_{23} = 0.40.\]</span></p>
<p>Let the column-vector <span class="math display">\[\mathbf{w}_{k}\]</span> represent the probabilities for a particular day’s weather and the column-vector <span class="math display">\[\mathbf{ w}_{k+1}\]</span>, the next day’s weather. The two are then related by:</p>
<p><span class="math display">\[
\mathbf{w}_{k+1} = M\mathbf{w}_{k}
\qquad (9)\]</span></p>
<p>Equation (<a href="#eq:recurrence">9</a>) is called a <a href="https://en.wikipedia.org/wiki/Recurrence_relation"><em>recurrence relation</em></a> or <em>difference equation</em>, and in our case, it represents the evolution of a dynamical system in time, namely the weather. Just for completeness, let the initial condition be given by:</p>
<p><span class="math display">\[
\mathbf{w}_{0} = \begin{bmatrix}0.55\\0.34\\0.11\end{bmatrix}
\qquad (10)\]</span></p>
<p>We want to know whether, for this model, there will be an equilibrium or steady-state in the weather, represented by a probability vector with values that remain steady with temporal evolution. The question is how do we find that out?</p>
<p>One obvious way is to compute the downstream weather one day at a time: think of forging a chain one link at a time because the weather has a memory of only one day. From equation(<a href="#eq:recurrence">9</a>) we can compute the following:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{w}_{1} &amp;= M\mathbf {w}_{0}\\
&amp;= \begin{bmatrix}0.47050\\ 0.33450\\0.19500\end{bmatrix}\\
\mathbf{w}_{2} &amp;= M\mathbf{w}_{1}\\
&amp;= M(M\mathbf{w}_{0})\\
&amp;= M^{2}\mathbf{w}_{0}\\
&amp;= \begin{bmatrix}0.42568\\0.34615\\0.228180\\\end{bmatrix}
\end{aligned}
\]</span></p>
<p>By induction, the weather vector <span class="math display">\[n\]</span> days downstream is given by</p>
<p><span class="math display">\[
\mathbf{w}_{n} = M^{n}\mathbf{w}_{0}.
\qquad (11)\]</span></p>
<p>In this manner, we can trace the time evolution of the weather and, if desired, draw a three-dimensional parametric plot of the successive weather vectors in <span class="math display">\[\mathbb{R}^{3}\]</span> with time as parameter. This could be insightful, but it is a laborious and time-consuming way to find out the steady-state weather vector if there is one. Could we do better?</p>
<p>A rough and ready method would be to evaluate equation (<a href="#eq:Mn">11</a>) with <span class="math display">\[n\]</span> set to large numbers, say <span class="math display">\[50\]</span> and <span class="math display">\[100\]</span>, and check if the resulting weather vectors, <span class="math display">\[\mathbf{w}_{50}\]</span> and <span class="math display">\[\mathbf{w}_{100}\]</span> were equal. If they were, we may safely identify that value as the steady-state weather vector.</p>
<p>But computing the fiftieth or one-hunderdth power of a matrix is tedious and error-prone if done by hand, and computationally expensive if done by machine, especially if the matrix in question is large.</p>
<p>To devise a better solution, we need to digress briefly to examine diagonal matrices and the diagonalization of square matrices.</p>
<h3 id="diagonal-matrix-raised-to-a-power">Diagonal matrix raised to a power</h3>
<p>Suppose that <span class="math display">\[D\]</span> is a <span class="math display">\[3 \times 3\]</span> diagonal matrix with non-zero entries on its principal diagonal and zeros elsewhere:</p>
<p><span class="math display">\[
D = \begin{bmatrix}
\lambda_{1} &amp; 0 &amp; 0\\
0 &amp; \lambda_{2} &amp; 0\\
0 &amp; 0 &amp; \lambda_{3}
\end{bmatrix}.
\]</span></p>
<p>Observe that:</p>
<p><span class="math display">\[
D^{n} = \begin{bmatrix}
\lambda_{1}^{n}&amp; 0 &amp; 0\\
0 &amp; \lambda_{2} ^{n}&amp; 0\\
0 &amp; 0 &amp; \lambda_{3}^{n}
\end{bmatrix}.
\qquad (12)\]</span></p>
<p>If we could somehow decompose <span class="math display">\[M\]</span> into a matrix product where a diagonal matrix was featured, we might be able to circumvent the matrix-raised-to-a large-power problem.</p>
<h3 id="matrix-diagonalization-or-eigen-decomposition">Matrix diagonalization or eigen decomposition</h3>
<p>We need to <a href="http://mathworld.wolfram.com/MatrixDiagonalization.html"><em>diagonalize</em></a> the transition matrix—a procedure called <a href="http://mathworld.wolfram.com/EigenDecomposition.html"><em>eigen decomposition.</em></a> A square matrix with non-repeating eigenvalues and therefore, linearly independent eigenvectors, can be diagonalized. We demonstrate how this is done for the <span class="math display">\[3 \times 3\]</span> case below.</p>
<p>Let the three eigenvectors be so denoted:</p>
<p><span class="math display">\[
\mathbf{e}_{k} = \begin{bmatrix}
e_{k1}\\
e_{k2}\\
e_{k3}
\end{bmatrix} ; k = 1, 2, 3.
\]</span></p>
<p>The matrix <span class="math display">\[P\]</span> whose <em>columns</em> are the eigenvectors is therefore <span class="math display">\[
P = \begin{bmatrix}
e_{11} &amp; e_{21} &amp; e_{31}\\
e_{12} &amp; e_{22} &amp; e_{32}\\
e_{13} &amp; e_{23} &amp; e_{33}
\end{bmatrix}
= \begin{bmatrix}
\mathbf{e}_{1} &amp; \mathbf{e}_{2} &amp; \mathbf{e}_{3}\\
\end{bmatrix}
\]</span></p>
<p>Now,</p>
<p><span class="math display">\[
\begin{aligned}
MP %
&amp;= M\begin{bmatrix}
\mathbf{e}_{1} &amp; \mathbf{e}_{2} &amp; \mathbf{e}_{3}
\end{bmatrix}\\
&amp;= \begin{bmatrix}
M\mathbf{e}_{1} &amp; M\mathbf{e}_{2} &amp; M\mathbf{e}_{3}
\end{bmatrix}\\
&amp;= \begin{bmatrix}
\lambda_{1}\mathbf{e}_{1} &amp; \lambda_{2}\mathbf{e}_{2} &amp; \lambda_{3}\mathbf{e}_{3}
\end{bmatrix}\\
&amp;= \begin{bmatrix}
e_{11} &amp; e_{21} &amp; e_{31}\\
e_{12} &amp; e_{22} &amp; e_{32}\\
e_{13} &amp; e_{23} &amp; e_{33}
\end{bmatrix}
\begin{bmatrix}
\lambda_{1} &amp; 0 &amp; 0\\
0 &amp; \lambda_{2} &amp; 0\\
0 &amp; 0 &amp; \lambda_{3}
\end{bmatrix}\\
&amp;= PD
\end{aligned}
\qquad (13)\]</span></p>
<p>To get only <span class="math display">\[M\]</span> on the left-hand side of equation (<a href="#eq:diag1">13</a>), we post-multiply both sides by <span class="math display">\[P^{-1}\]</span>, the inverse of <span class="math display">\[P\]</span>:</p>
<p><span class="math display">\[
MPP^{-1} = MI = M = PDP^{-1}
\qquad (14)\]</span></p>
<p>If we now square <span class="math display">\[M\]</span>, we get</p>
<p><span class="math display">\[
\begin{aligned}
M^{2} &amp;= (PDP^{-1})(PDP^{-1})\\
&amp;= PD(P^{-1}P)DP^{-1}\\
&amp;= P(DID)P^{-1}\\
&amp;= PD^{2}P^{-1}
\end{aligned}
\qquad (15)\]</span></p>
<p>By induction,</p>
<p><span class="math display">\[
M^{n} = PD^{n}P^{-1}
\qquad (16)\]</span></p>
<p>The role of eigenvalues and eigenvectors in the plot of raising a square matrix to a power is now fully revealed: recall that in equation (<a href="#eq:M-to-the-n">16</a>), <span class="math display">\[P\]</span> is the matrix whose columns are the eigenvectors, and <span class="math display">\[D\]</span> is the diagonal matrix whose non-zero elements are the corresponding eigenvalues, and <span class="math display">\[P^{-1}\]</span> is the inverse of <span class="math display">\[P.\]</span></p>
<h3 id="software-implementation">Software Implementation</h3>
<p>To get numerical results, I initially tried implementing the above steps with the free open-source mathematics software system <a href="http://www.sagemath.org/">SageMath,</a> but found it less than convenient for my purpose.</p>
<p>I then experimented with <a href="https://www.gnu.org/software/octave/">GNU Octave,</a> which is a free, open source, high-level interpreted language, primarily intended for numerical computations. It was better suited to the task at hand, and I easily obtained the results discussed below.</p>
<p>The self-explanatory file, <a href="http://swanlotus.org/wp-content/uploads/weather.m"><code>weather.m</code></a> may be downloaded and executed on the command line in the Octave command window. The discussion below will make better sense after you have thus executed the file <code>weather.m</code>. Because Octave is platform-neutral and downloadable free of charge, this is not a restrictive constraint.</p>
<h3 id="discussion-of-results-from-weather.m">Discussion of results from <code>weather.m</code></h3>
<p>The roots of the characteristic polynomial of <span class="math display">\[M\]</span> are first evaluated, and compared to the values of the eigenvalues and eigenvectors obtained from an Octave function designed for that explicit purpose.</p>
<p>There are <em>three</em> distinct eigenvalues for the transition matrix, <span class="math display">\[M.\]</span> Moreover, <a href="http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture1/lecture1.html">it may be proved</a> that <span class="math display">\[1\]</span> is an eigenvalue of any transition matrix. The eigenvalues we get are:</p>
<p><span class="math display">\[
\begin{aligned}
\lambda_{1} &amp;= 1.00000\\
\lambda_{2} &amp;= 0.48028\\
\lambda_{3} &amp;= 0.11972
\end{aligned}
\]</span></p>
<p>From equation(<a href="#eq:M-to-the-n">16</a>) we may surmise that the contributions from <span class="math display">\[\lambda_{2}\]</span> and <span class="math display">\[\lambda_{3}\]</span>, being both less than one, will diminish progressively as <span class="math display">\[n\]</span> increases, and in the limit, only the eigenvalue <span class="math display">\[1\]</span> will dominate the steady-state behaviour.</p>
<p>The eigenvectors associated respectively with these eigenvalues, as spewed out by Octave, are: <span class="math display">\[
\mathbf{e}_{1} = %
\begin{bmatrix}
-0.65375\\
-0.61639\\
-0.43895\\
\end{bmatrix}
\quad
\mathbf{e}_{2} = %
\begin{bmatrix}
-0.80430\\
0.28039\\
0.52391\\
\end{bmatrix}
\quad
\mathbf{e}_{3} = %
\begin{bmatrix}
0.37921\\
-0.81582\\
0.43662\\
\end{bmatrix}
\]</span></p>
<p>None of the column sums of these eigenvectors sums to one. Indeed, the column sums of <span class="math display">\[\mathbf{e}_{2}\]</span> and <span class="math display">\[\mathbf{e}_{3}\]</span> are close to zero, whereas the column sum of <span class="math display">\[\mathbf{e}_{1}\]</span> is negative. We will return to the eigenvector <span class="math display">\[\mathbf{e}_{1}\]</span>, to wrest meaning out of it, a little later. In any case, we confirm that <span class="math display">\[M\mathbf{e}_{1} = \mathbf{e}_{1}.\]</span></p>
<p>Assembling the matrices <span class="math display">\[P\]</span> and <span class="math display">\[P^{-1}\]</span> from the eigenvectors is trivial, as is putting together <span class="math display">\[D\]</span> as the diagonal matrix of the corresponding eigenvalues. With these numerical values, the truth of equation (<a href="#eq:eigendecomp">14</a>) is also easily demonstrated.</p>
<p>The time evolution of the initial weather vector is then tracked with 1, 10, 20, 50, and 100 iterations of equation (<a href="#eq:recurrence">9</a>). In this case, the weather vector stabilizes after about twenty iterations to a steady-state vector, <span class="math display">\[\mathbf{w}_{\infty}\]</span>, given by</p>
<p><span class="math display">\[
\mathbf{w}_{\infty} = %
\begin{bmatrix}
0.38251\\
0.36066\\
0.25683\\
\end{bmatrix}
\qquad (17)\]</span></p>
<p>When we track the same temporal evolution for eigenvector <span class="math display">\[\mathbf{e}_{1}\]</span>, the result after each iteration is <span class="math display">\[\mathbf{e}_{1}\]</span> itself. This is the expected behaviour for an eigenvector associated with an eigenvalue of <span class="math display">\[1.\]</span></p>
<p>What may be disconcerting, though, is that we now seem to have <em>two</em> steady-state vectors, <span class="math display">\[\mathbf{w}_{\infty}\]</span> and <span class="math display">\[\mathbf{e}_{1}.\]</span></p>
<p>Observe, however, that <span class="math display">\[\mathbf{e}_{1}\]</span> is not a probability vector whose columns sum to one. To convert it to a probability vector, we <em>normalize</em> <span class="math display">\[\mathbf{e}_{1}\]</span> by dividing it by its column sum, to get the <em>normalized eigenvector:</em></p>
<p><span class="math display">\[
\mathbf{n}_{1} = %
\begin{bmatrix}
0.38251\\
0.36066\\
0.25683\\
\end{bmatrix}
\qquad (18)\]</span></p>
<p>Lo and behold! <span class="math display">\[\mathbf{n}_{1}\]</span> and <span class="math display">\[\mathbf{w}_{\infty}\]</span> are one and the same, and all is well. Recalling from equation (<a href="#eq:scaled-eigenvector">7</a>) that non-zero scalar multiples of eigenvectors are also eigenvectors themselves, this result, even if a little magical, really should not surprise us.</p>
<p>We do not bother normalizing the eigenvectors associated with <span class="math display">\[\lambda_{2}\]</span> and <span class="math display">\[\lambda_{3}\]</span> because their column sums almost vanish, and moreover, their contribution to the steady state decreases with increasing number of iterations.</p>
<p>To round things off, we substitute a random initial weather vector in place of <span class="math display">\[\mathbf{w}_{0}\]</span>, and view its evolution over time for twenty iterations, and find that it too converges to the steady-state weather vector after about 15 iterations.</p>
<p>This means that regardless of what initial weather vector we start with, in about two weeks we will end up with a vector that represents the steady-state.</p>
<p>Observations like these suggest that our inferences are only as good as our assumptions and models. Oversimplification could lead to absurd results, and weather prediction over time is a seriously non-trivial problem.</p>
<p>One general hypothesis that we could examine is whether it is generally true that the normalized eigenvector associated with an eigenvalue of <span class="math inline">\(1\)</span> does indeed represent the steady state of the system represented by any transition matrix. If so, we would have a theorem on our hands.</p>
<p>A PDF version of this article is <a href="http://swanlotus.org/wp-content/uploads/eigen.pdf">available for download here.</a></p>

                <footer class="footer">
                    <div class="content has-text-centered">
                        <p>Copyright &copy; 2010 &mdash;
                            <script type="text/javascript">
                                document.write(new Date().getFullYear());
                            </script>, R (Chandra) Chandrasekhar | Powered by <a
                                href="http://getpelican.com">Pelican</a>
                        </p>
                    </div>
                </footer>
            </div>
            <!--Side Column-->
            <div id="sidebar" class="tile is-child is-3 is-size-5-tablet is-size-4-desktop">
                <ul>
                    <li><a href="http://localhost:8000/categories.html"><i class="fas fa-folder-open"></i> Categories</a</li>
                    <li><a href="http://localhost:8000/tags.html"><i class="fas fa-tags"></i> Tags</a></li>
                    <li><a href="http://localhost:8000/archives.html"><i class="fas fa-archive"></i> Archives</a></li>
                </ul>
            </div>
        </div>
    </div>
</body>